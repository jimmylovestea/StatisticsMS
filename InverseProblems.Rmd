---
title: "Inverse Problems in Experimental Particle Physics"
author: "Sean Gilligan"
output: 
  pdf_document:
    citation_package: biblatex
    number_sections: TRUE
keep_tex: TRUE
bibliography: citations.bib
header-includes:
  - \usepackage{setspace}\onehalfspacing
  - \usepackage{xcolor}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \usepackage{hyperref}
  - \usepackage{csquotes}
  - \usepackage{float}
  - \hypersetup{colorlinks=TRUE,linkcolor=red,citecolor=blue,filecolor=magenta,urlcolor=blue}
  - \newcommand{\comment}[1]{}
abstract: \singlespacing This report provides a survey of some of the common methods used by the high energy physics community to understand and solve ill-posed inverse problems as they pertain to signal distortions that result from imperfect measuring devices and processes. These methods are in general collectively referred to as unfolding. The specifics of data and data collection methods are generalized. Common features are discussed insofar as they contribute to the necessary understanding of the data and implementation of any covered unfolding methods. In order to construct a slightly more wholistic picture some additional topics are briefly touched upon if they relate to other common aspects of data analysis in particle physics, but only during parts of relevant discussions where they would otherwise normally appear.
fontsize: 12pt
---

```{r, message = F, echo=F}
library(tidyverse)
library(gridExtra)
library(grid)
library(gridtext)
library(knitr)
library(RColorBrewer)
library(viridisLite)
library(viridis)
library(gtools)
```


\section{Introduction}
A common problem faced in the quantitative sciences and their associated technologies is the introduction of errors during the data collection process. While the possible sources of these errors are as varied as the possible events which the data might describe, significant work has been done to develop methods the can help would-be analysts reconcile them. The requisite understanding of a scenario's underlying systematic and stochastic processes might not allow researchers to truly reverse entropy, but it can approximate it with a quantifiable degree of certainty. The applied mathematics that this involves falls within the general category of \textbf{inverse problems}, and there are a variety of labels used to refer to the procedures in its arsenal. Within the applications described here there is the colloquially vague \textbf{unsmearing}, but there are also names that reference specific applications and methods, such as those characterized in this report.

\subsection{The Deconvolution}

One way to characterize a basic example would be the following. Assume that data collected regarding $n$ statistical events represent the measurement of $n$ independent and identically distributed (i.i.d.) random variables $\bm{X}=\{X_1,X_2,\dots,X_n\}$ from a distribution of possible values represented by the probability density function (PDF) $f_X(x)$, such that the probability of a random variable $X_i$ having a value between $x_a$ and $x_b$ is $$P(\:x_a<X_i<x_b\:)=\int_{x_a}^{x_b}f(x)\,dx$$ and $$\int_\mathcal{X}f(x)\,dx=1,$$ where $\mathcal{X}$ represents the domain of $x$. The error introduced during the measurement process is similarly represented by a set of i.i.d. random variables $\bm{\varepsilon}=\{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n\}$ with a PDF $f_\varepsilon(\varepsilon)$, where the sets $\bm{\varepsilon}$ and $\bm{X}$ are typically assumed to be independent of each other. The set of measured values $\bm{Y}=\{Y_1,Y_2,\dots,Y_n\}$ then are also i.i.d. and can be defined in terms of the preceding sets of variables such that for event $i\in\{1,\dots,n\}$, 
\begin{align}Y_i&=g(X_i,\varepsilon_i)\nonumber\\&=X_i+\varepsilon_i.\label{eq:meas}\end{align} 
In light of this relationship, the corresponding PDF $f_Y(y)$ can be found explicitly through an operation on $f_X(x)$ and $f_\varepsilon(\varepsilon)$ using the mathematics of functional analysis. Stated in more general terms, the empirical density function $f_Y$ is formed from the \textbf{convolution} of the true density function $f_X$ and the error density function $f_\varepsilon$, and is defined by \cite{Panaretos2011} \begin{align}f_Y&\equiv f_X*f_\varepsilon\label{eq:conv1}\\f_Y(y)&\equiv\int_\mathcal{X}f_X(x)f_\varepsilon(\varepsilon)\,dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon\big(g_x^{-1}(y)\big)\left\vert J_{g_x^{-1}}(y)\right\vert dx\nonumber\\&=\int_\mathcal{X}f_X(x)f_\varepsilon(y-x)\,dx,\label{eq:conv2}\end{align}
where $J$ represents the Jacobian of the transformation involved in performing the change of basis on $f_\varepsilon$ from $\varepsilon$ to $x$, which is necessary for the evaluation of the integral for a given $y$. The magnitude of the Jacobian for transformation of $\varepsilon$ to $y-x$ through the manipulation of Equation \eqref{eq:meas} happens to be $1$.

As the collection of measured values $\bm{Y}$ accumulates an estimate of empirical density $\hat{f}_Y$ can readily be formed. However, a major goal in an analysis of data like this is typically to develop an accurate estimate of the true density $\hat{f}_X$. Using the information contained in $\hat{f}_Y$ to accomplish this necessarily requires some attempt at finding an inverse process to the convolution, i.e. the \textbf{deconvolution}.

For cases in the form of this particular example there are a variety approaches, but they commonly involve the Fourier transform of the density functions $\left\{f_X,f_\varepsilon,f_Y\right\}$ into their corresponding characteristic functions $\left\{\phi_X,\phi_\varepsilon,\phi_Y\right\}$ \cite{Meister2009}\cite{Panaretos2011}. Minor aspects of the definition for the Fourier transform can vary slightly between applications, resulting primarily from the use of different scale factors and sign conventions. Here it will be defined for some random variable $U\in\mathbb{R}$ with density function $f_U(u)$ and random variable $T\in\mathbb{R}$ as \begin{align}\phi_T(t)=\int_{-\infty}^\infty f_U(u)\,e^{itu}\,du.\label{eq:ft}\end{align} When conditions permit the inverse Fourier transform can be found via \begin{align}f_U(u)=\int_{-\infty}^\infty \phi_T(t)\,e^{-itu}\,dt.\label{eq:ift}\end{align} The Fourier transform is important in deconvolution methods because when you apply it to the convolution of two density functions the link between their respective characteristic functions becomes purely multiplicative, i.e.
$$f_Y=f_X*f_\varepsilon\implies\phi_Y=\phi_X\phi_\varepsilon.$$
An instructional proof of this result is provided on page 447 of \cite{Boas2005}.

\subsection{Generalizing}

The remainder of this paper is dedicated to characterizing the major methods adopted by the High Energy Physics (HEP) community toward solving their inverse problems. While most literature on deconvolution methods do use the word "convolution", this operation is also referred to by the German word \textit{faltung} \cite{Weisstein}. The latter's English translation, \textbf{folding}, is featured prominently in the particle physics community, but refers to a more generalized process than what is described by Equation \eqref{eq:conv2} \cite{DAgostini1994}\cite{Adye2011}\cite{Blobel2013}. In general, the terms folding and \textbf{unfolding} are used to describe two supersets of processes that respectively include convolution and deconvolution.

One way to arrive at the intended generalization is with the help of conditional probability. Thinking of $\{X,Y\}$ as a continuous bivariate random vector with joint PDF $f(x,y)$ and marginal PDFs $f_X(x)$ and $f_Y(y)$, we can define the conditional PDF of $Y$ given that $X=x$ as function of $y$, $f(y\,\vert x)$ \cite{Casella2001}. The relationship between these PDFs is sufficient to define any one of them in terms of operations involving one or more of the others. As such, for $f_Y(y)$ it can be shown
\begin{align}f_Y(y)&=\int_\mathcal{X}f(x,y)\,dx\nonumber\\&=\int_\mathcal{X}f(y\,\vert x)f_X(x)\,dx\nonumber\\&=\int_\mathcal{X}K(x,y)f_X(x)\,dx.\label{eq:fred}\end{align}
While integrating over $x$, $f(y\,\vert x)$ is implicitly treated a function of both $x$ and $y$. Acknowledging this allows for understanding Equation \eqref{eq:fred} as a Fredholm integral of the first kind with a Kernel function $K(x,y)$ reflecting the physical measurement process \cite{Blobel2011}. The relationship between $x$ and $y$ in $K(x,y)$ is not defined, but when the kernel is a function of the difference of its arguments, such that $K(x,y)=K(y-x)$, Equation \eqref{eq:fred} becomes the convolution described in Equation \eqref{eq:conv2}.

In particle physics experiments, analysts make use of Monte-Carlo (MC) simulations to estimate detector response to randoms samples from some true distribution $f_X(x)^{\text{MC}}$, which is itself estimated by way of MC simulations using models that typically contain theory being tested by the experiment in question. The resulting measured distribution $f_Y(y)^{\text{MC}}$ grants implicit knowledge of $K(x,y)$ by way of Equation \eqref{eq:fred} \cite{Blobel2013}. Finding the inverse of this Kernel is then the goal, as it should in theory allow for the mapping of experimental observations $\bm Y$, as randomly sampled from $f_Y(y)$, back to their true values $\bm X$.

\subsection{Discretization}

In practice researchers are only ever dealing with estimates $\hat f_X$, $\hat f_Y$, $\hat f_X^{\text{MC}}$, and $\hat f_Y^{\text{MC}}$, and the sets of data that contribute to these estimates are organized by bin into histograms that form unnormalized granular approximations of the true distributions. Thinking in terms of these histograms allows for the reformulation of Equation \eqref{eq:fred} into the linear matrix equation:
\begin{align}\bm\nu = \bm{R}\bm\mu.\label{eq:mat}\end{align}
The vectors $\bm\nu$, $\bm\mu$ and matrix $\bm{K}$ are mapped from their continuous counterparts by \cite{Blobel2013}:
\begin{align}
  \text{true distribution }f_X(x)&\longrightarrow\bm\mu\,\in\,\mathbb{R}^N_{+}\cup\bm{0}\text{ the unknown true bin counts,}\nonumber\\
  \text{measured distribution }f_Y(y)&\longrightarrow\bm\nu\,\in\,\mathbb{R}^M_{+}\cup\bm{0}\text{ the measured bin bounts,}\nonumber\\
  \text{Kernel }K(x,y)&\longrightarrow\bm{R}\;\;\text{rectangular }M\text{-by-}N\text{ \bf response matrix}\text{.}\nonumber
\end{align}
The components of vectors $\bm\nu$ and $\bm\mu$ represent the number of events that have occurred within the regions of $x$ and $y$ that define the components' corresponding bins. For $i=1,\dots,M$ and $j=1,\dots,N$ the components of matrix $\bm R$ are defined by the conditional probability \cite{Cowan1998}
\begin{align}
  R_{ij}&=P(\text{measured value in bin }i\vert\text{true value in bin }j)\nonumber\\
        &=\frac{P(\text{measured value in bin }i\text{ and true value in bin }j)}{P(\text{true value in bin }j)}\nonumber\\
        &=\frac{\int_{\text{bin }i}\int_{\text{bin }j}K(x,y)f_X(x)dx\,dy}{\int_{\text{bin }j}dx\,f_X(x)}\nonumber\\
        &\equiv P(\nu_i\vert\mu_j).\label{eq:Rij}
\end{align}
In terms of $P(\nu_i\vert\mu_j)$ the full response matrix then has the form
\begin{align}
  \bm{R}=\begin{pmatrix}
    P(\nu_1\vert\mu_1)     & P(\nu_1\vert\mu_2)     & \dots  & P(\nu_1\vert\mu_{N})   \\
    P(\nu_2\vert\mu_1)     & P(\nu_2\vert\mu_2)     & \cdots & P(\nu_2\vert\mu_{N})   \\
    \vdots                 & \vdots                 & \ddots & \vdots                 \\
    P(\nu_{M}\vert\mu_1)   & P(\nu_{M}\vert\mu_2)   & \dots  & P(\nu_{M}\vert\mu_{N})
  \end{pmatrix}.\label{eq:Rmat}
\end{align}
With these definitions Equation \eqref{eq:mat} tells us that an event produced in bin $\mu_j$ has some probability $\geq 0$ of being measured in each of the $M$ bins of $\bm\nu$, and that each bin count $\nu_i$ receives potential contributions from each of the $N$ bins in $\bm\mu$, i.e. 
\begin{align}\nu_i = \sum_{j=1}^NR_{ij}\mu_j.\label{eq:bini}\end{align}
The number of bins are typically set such that $N\leq M$, with the convention $M=N+1$ being common. A higher number of bins in the measured distribution reflects that the measuring process is expected to map some events in $\bm X$ to values of $\bm Y$ that are outside the region of values that define the initial $N$ bins. These one or more extra bins are intended to account for all the possible values that a particular event could be mapped to, such that for a given event starting in bin $j$ one might expect the probabilities of it being measured in each of the $M$ final bins to sum to $1$. 

However, in practice there are a variety of constraints on events that can either result in them not being included for analysis or even prevent them from being detected at all. For example, an analyst might cut events observed in regions of a detector that result in insufficient data collection, or maybe some event information carriers miss the detector entirely, resulting in such events going unseen. In either case the effect of these missing events is described using the detector \textbf{efficiency}, and represented mathematically by the $M$-vector $\bm\epsilon$, where component $\epsilon_j$ is the efficiency of the $j$th true bin defined\footnote{In the continuous case it is typically written as $\epsilon(x)$, and understood to be the conditional probability of an event producing any measured value given it has a true value of $x$. It is typically absorbed into $K(x,y)$ where it goes on to manifest within $\bm R$ in the manner shown in Equation \eqref{eq:eff} \cite{Blobel2013}.} by \cite{Cowan1998}:
\begin{align}\sum_{i=1}^{M}P(\nu_i\vert\mu_j)=\sum_{i=1}^{M}R_{ij}=\epsilon_j\leq 1.\label{eq:eff}\end{align}
In contrast to this are contributions to measured counts from \textbf{background} processes, which are typically studied separately and not involved directly in the unfolding process. Ignoring it would be a major omission in any description of HEP data, so it is briefly included here for completion. Mathematically it is included by modifying Equation \eqref{eq:bini} to read
\begin{align}\nu_i = \sum_{j=1}^NR_{ij}\mu_j+\beta_i,\end{align}
where $\beta_i$ is the $i$th component of the $M$-vector $\bm\beta$, which represents the binned background counts. This leads to equations like $\nu_i^{\text{sig}}=\nu_i-\beta_i$ in order to specify the expected number of measured counts that are from the signal of interest. Going forward background will be assumed to already have been accounted for, and $\nu_i$ will refer to the measured signal counts of bin $i$.

As all these variables so far have been derived from the exact continuous distributions $f_X(x)$ and $f_Y(y)$, they correspond to the expectation values that researchers are estimating during data collection and analysis. As this is a counting process the components of the observed number of signal events $\bm{n}$, an $M$-vector, are often related to the components of the expected number of observed counts $\bm\nu$ as a collection of $M$ separate and independent Poisson processes. That is to say the observed counts $n_i$ in bin $i$ are treated as i.i.d. random variables with the probability mass function
\begin{align}P(n_i\vert\nu_i)=\frac{\nu_i^{n_i}e^{-\nu_i}}{n_i!}.\label{eq:pois}\end{align}
The counts $n_i$ would in theory then form the estimate $\hat\nu_i$ of the expected counts $\nu_i$ by
\begin{align}\nu_i&=\text{E}[\hat\nu_i]=\text{E}[n_i]\nonumber\\&=\text{Var}[\hat\nu_i]=\text{Var}[n_i].\nonumber\end{align}
Understanding the probability distribution of $\bm{n}$ allows for unfolding methods that involve the use of maximum likelihood estimation. For methods based on least squares it becomes necessary to find the covariance matrix $\bm{V}$ of the observations, which for independent Poisson processes has components of the form
\begin{align}V_{ij}&=\text{Cov}[n_i,n_j]\nonumber\\&=\delta_{ij}\nu_i,\label{eq:cov}\end{align}
where $\delta_{ij}$ is the Kronecker delta\footnote{The Kronecker delta $\delta_{ij}$ is a piecewise function of variables $i$ and $j$ defined by $\delta_{ij}=\begin{cases}0\;\;\;\text{if }i\neq j\\1\;\;\;\text{if }i=j\end{cases}.$}.

\subsection{A Simulated Example}

Consider an example of the form described by Equation \eqref{eq:meas}, i.e. $Y_i=\epsilon_i\left(X_i+\varepsilon_i\right)$. Let $X_i$ be a i.i.d. random variable from a bimodal distribution of the form $X_i=Z_iX_{1,i}+(1-Z_i)X_{2,i}$, where
\begin{align}
  X_{1,i}&\sim \text{Gamma}(24,0.4),\nonumber\\
  X_{2,i}&\sim \text{Gamma}(42,0.4),\nonumber\\
  \text{and }Z_i&\sim\text{Bernoulli}(2/7),\nonumber
\end{align}
and let the effects of detector smearing be represented by i.i.d random variables generated by the conditional Gaussian process $\varepsilon_i\sim N\left(\mu\small{(}X_i\small{)},\sigma\small{(}X_i\small{)}^2\right)$, the mean and variance of which are functions defined by
\begin{align}
  \mu(X_i=x)&=-x^{1/4}\;\;\text{and}\nonumber\\
  \sigma(X_i=x)&=\log\left(\frac{x+10}{4}\right).\nonumber
\end{align}
The efficiency is similarly conditional on $X_i$, and is modeled here as a Bernoulli process with i.i.d random variables $\epsilon_i\sim\text{Bernoulli}\big(p(X_i)\big)$, where the average detection rate (when $\epsilon_i=1$) is a function of the form
\begin{align}
  p(X_i=x)=1-e^{-\sqrt{x}/4}.\nonumber
\end{align}

\begin{figure}[!ht]
    \centering
    \hrulefill
```{r, echo = F}
set.seed(1234)
# Assume 10000 events
nsim <- 10000
mcsim <- 10*nsim

# Two possible types of processes with different means but equal variances
alpha1 <- 24
alpha2 <- 42
beta <- 0.4

# Event 1 is 1/3 as likely to occur as event 2
p <- 2/7

x1 <- rgamma(nsim, shape = alpha1, scale = beta)
x2 <- rgamma(nsim, shape = alpha2, scale = beta)
x1mc <- rgamma(mcsim, shape = alpha1, scale = beta)
x2mc <- rgamma(mcsim, shape = alpha2, scale = beta)

z <- rbernoulli(nsim, p)
x <- c(x1[z],x2[!z])
zmc <- rbernoulli(mcsim, p)
xmc <- c(x1mc[zmc],x2mc[!zmc])

# Efficiency
xdetected <- rbernoulli(length(x),1-exp(-sqrt(x)/4)) == 1
xdetectedmc <- rbernoulli(length(xmc),1-exp(-sqrt(xmc)/4)) == 1

# Add Smearing
ermu <- -x^(1/4)
ersd <- log((x+10)/4)
y <- x + rnorm(nsim, mean = ermu, sd = ersd)

ermumc <- -xmc^(1/4)
ersdmc <- log((xmc+10)/4)
ymc <- xmc + rnorm(mcsim, mean = ermumc, sd = ersdmc)

# Binned representation
xymin <- 4
xymax <- 26
xystep <- 3
dxy <- 4
xyaxes <- c(xymin-dxy,xymin,
            seq(xymin+xystep,xymax-xystep,xystep),
            xymax,xymax+dxy)

sims <- tibble("Truth" = x,
               "Measured" = y,
               "Detected" = xdetected)

simsmc <- tibble("Truth" = xmc,
                 "Measured" = ymc,
                 "Detected" = xdetectedmc)

step_hist_notfolded <- hist(x[which(x > xymin-dxy & x < xymax+dxy)], 
                            breaks = seq(xymin-dxy,xymax+dxy,1), plot = F)
step_hist_folded <- hist(y[which(y > xymin-dxy & y < xymax+dxy & xdetected == TRUE)], 
                         breaks = seq(xymin-dxy,xymax+dxy,1), plot = F)

fmax <- 1.075*max(c(step_hist_folded$density,step_hist_notfolded$density))
fmax_count <- 1.05*max(c(step_hist_folded$count,step_hist_notfolded$count))

step_hist <- 
  tibble(binLow = c(step_hist_folded$breaks[-length(step_hist_folded$breaks)],
                    step_hist_notfolded$breaks[-length(step_hist_notfolded$breaks)]),
         binHigh = c(step_hist_folded$breaks[-1],step_hist_notfolded$breaks[-1]),
         CountsL = c(c(0,step_hist_folded$counts[-length(step_hist_folded$counts)]),
                     c(0,step_hist_notfolded$counts[-length(step_hist_notfolded$counts)])),
         Counts = c(step_hist_folded$counts,step_hist_notfolded$counts),
         DensityL = c(c(0,step_hist_folded$density[-length(step_hist_folded$density)]),
                      c(0,step_hist_notfolded$density[-length(step_hist_notfolded$density)])),
         Density = c(step_hist_folded$density,step_hist_notfolded$density),
         Treatment = c(rep("folded", length(step_hist_folded$density)),
                       rep("not folded", length(step_hist_notfolded$density))))
```

```{r, echo = F}
# Continuous representation
X <- seq(xymin-dxy,xymax+dxy,0.01)
nx <- length(X)

fx = p*dgamma(X, shape = alpha1, scale = beta) + 
     (1-p)*dgamma(X, shape = alpha2, scale = beta)
#fx <- p*dlnorm(X, meanlog = 2.25, sdlog = 0.25) +
#  (1-p)*dlnorm(X, meanlog = 2.8, sdlog = 0.15)

fx_data <- tibble(X = X, Density = fx)
# fy_data content calculated in fyEstimate.nb
fy_data <- read.csv("fyEstimate.csv")

# Binned expected counts
exp_hist <- read.csv("histExpected.csv")
exp_hist[,3:4] <- nsim*exp_hist[,3:4]

# Counts max
fmax_count <- max(fmax_count, 1.05*max(nsim*fx_data$Density))
```

```{r, fig.height=4.8, fig.width=7.5, fig.align='center', echo = F}
# Plotting
comp1 <- ggplot(data = step_hist) + 
  theme_bw() +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "E[Binned Truth]",
                      linetype = "E[Binned Truth]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "E[Binned Truth]",
                      linetype = "E[Binned Truth]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "E[Binned Measured]",
                      linetype = "E[Binned Measured]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "E[Binned Measured]",
                      linetype = "E[Binned Measured]")) +
  geom_line(data = filter(fx_data,
                            X >= xymin-dxy &
                            X <= xymax+dxy),
            mapping = aes(x = X, 
                          y = nsim*Density,
                      color = "Scaled PDF Truth",
                   linetype = "Scaled PDF Truth")) +
  geom_line(data = filter(fy_data,
                            Y >= xymin-dxy &
                            Y <= xymax+dxy),
            mapping = aes(x = Y, 
                          y = nsim*Density,
                      color = "Scaled PDF Measured",
                   linetype = "Scaled PDF Measured")) +
  scale_x_continuous(breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),
                                  floor(fmax_count/1000)*200)) +
  scale_color_manual(name = NA,
                     labels = c("E[Binned Truth]",
                                "Scaled PDF Truth",
                                "E[Binned Measured]",
                                "Scaled PDF Measured"),
                     breaks = c("E[Binned Truth]",
                                "Scaled PDF Truth",
                                "E[Binned Measured]",
                                "Scaled PDF Measured"),
                     values = c("E[Binned Truth]" = alpha("#009E73",1),#alpha("#4C8C2B",0.6),
                                "Scaled PDF Truth" = alpha("#009E73",0.6),#alpha("#4C8C2B",1),
                                "E[Binned Measured]" = alpha("#0072B2",1),#alpha("#004C97",0.6),
                                "Scaled PDF Measured" = alpha("#0072B2",0.6))) +#alpha("#004C97",1))) +
  scale_linetype_manual(name = NA,
                        labels = c("E[Binned Truth]",
                                   "Scaled PDF Truth",
                                   "E[Binned Measured]",
                                   "Scaled PDF Measured"),
                        breaks = c("E[Binned Truth]",
                                   "Scaled PDF Truth",
                                   "E[Binned Measured]",
                                   "Scaled PDF Measured"),
                        values = c("E[Binned Truth]" = "solid",
                                   "Scaled PDF Truth" = "solid",
                                   "E[Binned Measured]" = "solid",
                                   "Scaled PDF Measured" = "solid")) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.01, "cm"),
        legend.text = element_text(size = 8),
        legend.title = element_blank(),
        legend.position = c(.01, 0.99),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        axis.title.x = element_blank()) +
  guides(color = guide_legend(override.aes = list(alpha = c(1,0.15,1,0.15)))) +
  geom_rect(data=tibble(x1=xymin-dxy,x2=xymin,y1=0,y2=fmax_count),
            mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
            alpha=0.5,size=0.3) +
  geom_rect(data=tibble(x1=xymax,x2=xymax+dxy,y1=0,y2=fmax_count),
            mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
            alpha=0.5,size=0.3)

comp2 <- ggplot(data = step_hist) + 
  theme_bw() +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy ),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "E[Binned Truth]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "E[Binned Truth]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "E[Binned Measured]")) +
  geom_segment(data = filter(exp_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "E[Binned Measured]")) +
  geom_segment(data = filter(step_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "Binned Truth")) +
  geom_segment(data = filter(step_hist, 
                             Treatment == "not folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "Binned Truth")) +
  geom_segment(data = filter(step_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                         color = "Binned Measured")) +
  geom_segment(data = filter(step_hist, 
                             Treatment == "folded" &
                                binLow >= xymin-dxy &
                               binHigh <= xymax+dxy),
               mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                         color = "Binned Measured")) +
  scale_x_continuous(breaks = xyaxes, 
                     limits = c(xymin-dxy,xymax+dxy), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts",
                     limits = c(0,fmax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(fmax_count),
                                  floor(fmax_count/1000)*200)) +
  scale_color_manual(name = NA,
                     breaks = c("E[Binned Truth]",
                                "Binned Truth",
                                "E[Binned Measured]",
                                "Binned Measured"),
                     values = c("E[Binned Truth]" = alpha("#009E73",1),
                                "Binned Truth" = alpha("#E69F00",1),#alpha("#CB6015",0.7),
                                "E[Binned Measured]" = alpha("#0072B2",1),#alpha("#004C97",0.7),
                                "Binned Measured" = alpha("#D55E00",1)))+#alpha("#8A2A2B",0.7))) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.margin = margin(0.15, 0.16, 0.12, 0.01, "cm"),
        legend.text = element_text(size = 8),
        legend.title = element_blank(),
        legend.position = c(.01, 0.99),
        legend.justification = c("left", "top"),
        legend.box.just = "left",
        legend.key.height = unit(10,"points"),
        legend.margin = margin(1,5,5,5,"pt"),
        axis.title.x = element_blank()) +
  geom_rect(data=tibble(x1=xymin-dxy,x2=xymin,y1=0,y2=fmax_count),
            mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
            alpha=0.5,size=0.3) +
  geom_rect(data=tibble(x1=xymax,x2=xymax+dxy,y1=0,y2=fmax_count),
            mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
            alpha=0.5,size=0.3) #+
  #guides(color = guide_legend(override.aes = list(alpha = 0.2)))

migration <- sims %>%
  mutate(across(Detected, factor, levels = c("TRUE","FALSE"))) %>%
  filter(Truth <= xymax+dxy & Truth >= xymin-dxy &
           Measured <= xymax+dxy & Measured >= xymin-dxy) %>%
  ggplot() +
    scale_x_continuous(breaks = xyaxes,
                       limits = c(xymin-dxy,xymax+dxy), 
                       expand = c(0,0)) +
    scale_y_continuous(breaks = xyaxes, 
                       limits = c(xymin-dxy,xymax+dxy), 
                       expand = c(0,0)) +
    theme_bw() + theme(panel.grid.major = element_blank(), 
                       panel.grid.minor = element_blank()) +
    stat_bin2d(mapping = aes(x = Truth, 
                             y = Measured), 
               breaks = list(x = seq(xymin-dxy,xymax+dxy,1),
                             y = seq(xymin-dxy,xymax+dxy,1))) +
    labs(x = "X (Truth)", y = "Y (Measured)",
         title = "Event Detected") +
    geom_rect(data=tibble(x1=xymin-dxy,x2=xymax+dxy,y1=xymin-dxy,y2=xymin),
              mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
              alpha=0.5,size=0.3) +
    geom_rect(data=tibble(x1=xymin-dxy,x2=xymin,y1=xymin-dxy,y2=xymax+dxy),
              mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
              alpha=0.5,size=0.3) +
    geom_rect(data=tibble(x1=xymin-dxy,x2=xymax+dxy,y1=xymax,y2=xymax+dxy),
              mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
              alpha=0.5,size=0.3) +
    geom_rect(data=tibble(x1=xymax,x2=xymax+dxy,y1=xymin-dxy,y2=xymax+dxy),
              mapping=aes(xmin=x1,xmax=x2,ymin=y1,ymax=y2),
              alpha=0.5,size=0.3) +
    geom_abline(intercept=0,slope=1,lty="dashed",col="red") +
    facet_wrap(Detected~., nrow = 2, scales = "free")

gradient_max <- max(ggplot_build(migration)$data[[1]]$count)
gmins <- abs(floor(gradient_max/10^(floor(log10(gradient_max))-1))/
               c(5,10,20,25,50)-1)
gradient_step <- c(5,10,20,25,50)[which(gmins == min(gmins))]*
  10^(floor(log10(gradient_max))-2)

migration <- migration +
  scale_fill_gradientn(colours = viridis(11),
                       breaks = seq(0,gradient_max,gradient_step),
                       guide = guide_colourbar(barwidth = 0.75, 
                                               barheight = 15,
                                               title = "Counts"))

grid.arrange(arrangeGrob(comp1, comp2, richtext_grob("X (Truth), Y (Measured)"),
                         heights = c(10,10,1)), 
             migration, ncol = 2, widths = c(6,5))

rmu <- sims %>%
  filter(Truth <= xymax+dxy & Truth >= xymin-dxy) %>%
  select(Truth) %>%
  mutate(Truth = ceiling(Truth)-floor(min(Truth))) %>%
  count(Truth, name = "counts") %>%
  pull(counts)

rR <- sims %>%
  filter(Truth <= xymax & Truth >= xymin &
           Measured <= xymax & Measured >= xymin &
           Detected == "TRUE") %>%
  select(Measured, Truth) %>%
  mutate(Measured = ceiling(Measured)-floor(min(Measured)),
         Truth = ceiling(Truth)-floor(min(Truth))) %>%
  count(Measured, Truth, name = "counts") %>%
  complete(Measured, Truth, fill = list(counts=0)) %>%
  pull(counts) %>%
  matrix(ncol = xymax-xymin, byrow = T)

#step_hist
dp_hist <- sims %>%
  filter(Truth <= xymax+dxy & Truth >= xymin-dxy &
           Measured <= xymax+dxy & Measured >= xymin-dxy) %>%
  pivot_longer(c(Measured,Truth), 
               names_to = "Treatment", 
               values_to = "binLow") %>%
  filter((Treatment == "Measured" & Detected == TRUE) | Treatment == "Truth") %>%
  mutate(binHigh = ceiling(binLow),
         binLow = floor(binLow)) %>%
  count(binHigh, Treatment, name = "Counts") %>%
  mutate(Density = Counts/nsim)

```
  \caption{\emph{The above plots feature two bimodal gamma distributions depicting events before and after after detector effects. (Top Left) The two continuous distributions correspond to the theoretical PDFs of the two distributions rescaled to correspond with the counts from 10,000 events. The histograms are calculated from the PDFs and correspond to the expected event counts from 10,000 simulated events. (Bottom Left) These four histograms consist of the same expected event counts as well as one instance of actual counts resulting from 10,000 simulated events. (Right) A visual study of simulated detector efficiency is provided by a side-by-side comparison of two heat maps that demonstrate the skewness and dispersion added by a simulated measurement process for detected and undetected events. This study is not intended to meaningfully represent a hypothetical distribution of undetected events in any real detector. Actual detector efficiencies are almost certainly governed by much more complicated collections of parameters.}}
  \label{BasicExample}
  \hrulefill
\end{figure}

```{r, echo=F}
dp_hist <- sims %>%
  pivot_longer(c(Measured,Truth), 
               names_to = "Treatment", 
               values_to = "binHigh") %>%
  mutate(binHigh = ceiling(binHigh)) %>%
  filter(binHigh <= 30 & binHigh >= 1) %>%
  filter((Treatment == "Measured" & Detected == TRUE) | Treatment == "Truth") %>%
  count(binHigh, Treatment, name = "Counts") %>%
  complete(binHigh=1:30, Treatment, fill = list(Counts=0)) %>%
  mutate(binLow = binHigh-1,
         LCounts = rbind(diag(rep(1,60))[1:2,],diag(rep(1,60))[1:58,]) %*% Counts) %>%
  mutate(Density = Counts/mcsim,
         LDensity = LCounts/mcsim) %>%
  select(Treatment,binLow,binHigh,LCounts,Counts,LDensity,Density)

mc_bins <- simsmc %>%
  pivot_longer(c(Measured,Truth), 
               names_to = "Treatment", 
               values_to = "Bin") %>%
  mutate(Bin = ceiling(Bin)) %>%
  filter(Bin <= 30 & Bin >= 1) %>%
  filter((Treatment == "Measured" & Detected == TRUE) | Treatment == "Truth") %>%
  count(Bin, Treatment, name = "Counts") %>%
  complete(Bin=1:30, Treatment, fill = list(Counts=0)) %>%
  mutate(LBin = Bin-1,
         LCounts = rbind(diag(rep(1,60))[1:2,],diag(rep(1,60))[1:58,]) %*% Counts) %>%
  mutate(Density = Counts/mcsim,
         LDensity = LCounts/mcsim) %>%
  select(Treatment,LBin,Bin,LCounts,Counts,LDensity,Density)

dnu <- dp_hist %>%
  filter(Treatment == "Measured") %>%
  select(binHigh,Counts) %>%
  mutate(Good = Counts>50)
dmu <- dp_hist %>%
  filter(Treatment == "Truth") %>%
  select(binHigh,Counts) %>%
  mutate(Good = Counts>50)

mcnu <- mc_bins %>%
  filter(Treatment == "Measured") %>%
  select(Bin, Counts)
mcmu <- mc_bins %>%
  filter(Treatment == "Truth") %>%
  select(Bin, Counts)

mR <- as.matrix(read.csv("migrationMatrix.csv", header = F))
colnames(mR) <- NULL
mR <- mR[dnu$Good,dmu$Good]
mR <- zapsmall(mR, digits = 6)

mRsvd <- svd(mR, nu = dim(mR)[1], nv = dim(mR)[2])

Sig <- zapsmall(t(mRsvd$u) %*% mR %*% mRsvd$v, digits=6)
invSig <- t(rbind(diag(1/mRsvd$d),matrix(rep(0,(dim(mR)[1]-dim(mR)[2])*dim(mR)[2]),ncol=dim(mR)[2])))

enu <- exp_hist %>%
  filter(Treatment == "folded") %>%
  pull(Counts)

muSVD <- mRsvd$v %*% invSig %*% t(mRsvd$u) %*% (dnu$Counts[dnu$Good]/(enu[dnu$Good]))

#plot(x=c(-5,35),y=c(0,0), xlim = c(5,25), ylim = c(min(muSVD)-10,max(muSVD,dmu$Counts)+10), type = "l", xlab = "X", ylab = "Counts", main="SVD in progress, no regularization")
#points((1:30)[dmu$Good]-0.5, muSVD, col = 2)
#lines(rbind((1:30)[dmu$Good]-1,(1:30)[dmu$Good]), rep(muSVD,each=2), col = 2)
#points((1:30)[dmu$Good]-0.5, dmu$Counts[dmu$Good], col = 4)
#lines(rbind((1:30)[dmu$Good]-1,(1:30)[dmu$Good]), rep(dmu$Counts[dmu$Good],each=2), col = 4)


#vals <- rep(0,15)
#for (i in 1:15){
#  vals[i] <- sqrt(sum((zapsmall(mR,digits=i) %*% dmu$Counts - mR %*% dmu$Counts)^2))
#}
#plot(1:15, log10(vals))
#cbind(1:15,vals)

#mR %*% (step_hist %>% filter(Treatment == "not folded") %>% pull(Counts)) - step_hist %>% filter(Treatment == "folded") %>% pull(Counts)

#length(step_hist %>% filter(Treatment == "not folded") %>% pull(Counts))

#sum(mc_hist %>% filter(Treatment == "Truth") %>% pull(Counts))


Rmc <- (simsmc %>%
  filter(Detected == TRUE,
         Measured >= 0 & Measured <= 30) %>%
  mutate(Truth = ceiling(Truth),
         Measured = ceiling(Measured)) %>%
  count(Truth, Measured, name = "Counts") %>%
  complete(Measured=30:1, Truth=1:30, fill = list(Counts=0)) %>%
  pull(Counts) %>%
  matrix(byrow = T, ncol = 30)) %*% (simsmc %>%
  mutate(Truth = ceiling(Truth)) %>%
  select(Truth) %>%
  count(Truth, name = "Counts") %>%
  mutate(Rscale = 1/Counts) %>%
  complete(Truth = 1:30, 
           fill = list(Counts=0,Rscale=0)) %>%
  pull(Rscale) %>% diag)

plot(1:30, Rmc %*% mcmu$Counts, pch = 15, col = "red")
points(1:30, mcnu$Counts, pch = 5, col = "blue")
```
```{r}
Rmc
svd(Rmc, nu = dim(Rmc)[1], nv = dim(Rmc)[2])
(svdRmc <- svd(Rmc[,4:30], nu = dim(Rmc[,4:30])[1], nv = dim(Rmc[,4:30])[2]))
```

\section{Unfolding methods in particle physics}

\subsection{Bin-by-bin}

In this approach a multiplicative \textbf{correction factor} $C_i$ is applied to the observed number of signal events $n_i$ for each bin to produce the estimator of $\mu_i$ \cite{Cowan1998},
\begin{align}
  \hat{\mu}_i &= C_i(n_i-\beta_i)\nonumber\\
  &= C_i\mu_i.\label{eq:binest}
\end{align}
The correction factors are determined by taking the respective ratios of a bin's MC simulated truth signal event counts $\mu_i^{\text{MC}}$ to its MC simulated reconstructed signal event counts $\nu_i^{\text{MC}}$,
\begin{align}C_i = \frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}.\label{eq:cfact}\end{align}
The covariance matrix $\bm{U}$ of this estimator derives naturally from Equations \eqref{eq:cov} and \eqref{eq:binest}, with components
\begin{align}
  U_{ij}&=\text{Cov}[\hat\mu_i,\hat\mu_j]\nonumber\\
  &=C_iC_j\text{Cov}[n_i,n_j]\nonumber\\
  &=C_i^2\delta_{ij}\nu_i.\label{eq:bincov}
\end{align}
The expectation value of the estimate can be calculated easily enough as well, and with it the bias 
\begin{align}
  \text{Bias}[\hat{\mu}_i]&=E_i[\hat{\mu}_i]-\mu_i\nonumber\\ 
  &= C_iE[n_i]-\mu_i\nonumber\\ 
  &= \frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}\nu_i-\mu_i\nonumber\\
  &= \left(\frac{\mu_i^{\text{MC}}}{\nu_i^{\text{MC}}}-\frac{\mu_i}{\nu_i}\right)\nu_i.\label{eq:binbias}
\end{align}



\section{Algorithm Construction}

Unfolding is ultimately concerned with finding a reliable inverse to the process by which an event occurring in some true bin $i$ maps to an observed bin $j$. In generalizing this a bit we can think in terms of \textit{causes}, $C_i$ ($i=1,\dots,n_C$) and \textit{effects}, $E_j$ ($j=1,\dots,n_E$), representing the true and observed bins respectively. In regard to a single event we are then interested in conditional probabilistic view for causation, $P(C_i\vert E_j,I)\equiv\theta_{ij}$, the probability that we can attribute some cause $C_i$ to an observed effect $E_j$. Using Bayes' theorem we can define this in terms of other probabilities that can be estimated more directly,
\begin{eqnarray}
P(C_i\vert E_j,I) & = & \frac{P(E_j\vert C_i,I)\cdot P(C_i\vert I)}{\sum_{i=1}^{n_C}P(E_j\vert C_i,I)\cdot P(C_i\vert I)}\nonumber\\
\theta_{ij} & = & \frac{\lambda_{ji}\cdot P(C_i\vert I)}{\sum_{i=1}^{n_C}\lambda_{ji}\cdot P(C_i\vert I)},
\end{eqnarray}
where the conditional probability regarding inference (effect), $P(E_j\vert C_i,I)\equiv\lambda_{ji}$, is the probability that some effect $E_j$ will result with some cause $C_i$ and $P_o(C_i\vert I)$ is the true probability of an event occurring from cause $C_i$. The term $I$ represents any implicit conditional information regarding the analysis, such as the choice of prior, and is usually apparent when the probabilities are written out as density functions.

At the analysis level we care less about individual events and more about mapping the total number events per effect, $\bm{x}_E=\{x(E_1),\dots,x(E_{n_E})\}$, to the total number of events per cause, $\bm{x}_C=\{x(C_1),\dots,x(C_{n_C})\}$. However, so far this regards only observed events as categorized into the $n_E$ effects, as we cannot expect to observe or select for all effects resulting from some arbitrary cause $C_i$. In light of this, while we can add causes to account for any independent background sources to assume the normalization of $P_o(C_i\vert E_j,I)$ and $P_o(C_i\vert I)$, such that $\sum_{i=1}^{n_C}P_o(C_i\vert E_j, I)=1$ and $\sum_{i=1}^{n_C}P(C_i\vert I)=1$, we cannot say the same for $P(E_j\vert C_i,I)$. A necessarily imperfect effect selection capability results in 
$$0\leq\sum_{j=1}^{n_E}P(E_j\vert C_i,I)=\sum_{j=1}^{n_E}\lambda_{ji}\equiv\epsilon_i\leq 1,$$ 
the exact value of which provides for us a definition for $\epsilon_i$, the \textit{efficiency} at which we detect cause $C_i$ from all accounted for observed effects, being also defined and useable as the ratio of observed events resulting from cause $C_i$ to the true number of events resulting from $C_i$, $x^{obs}(C_i)/x(C_i)$.

```{r, echo = FALSE, fig.align='center', fig.cap="\\label{c2elinks}\\emph{Thinking of causes and effects as distinct subsets within one or more dimensional cause and effect phase spaces, this figure shows how events are probabilistically mapped from the subsets used to define our causes to the subsets used to define our effects. The node indicated by $T$ (`trash') represents the event selection inefficiency, and can be thought of as an additional effect $E_{n_E+1}$ that contains an unobserved number of events. Unlike the possibility of alloting independent sources of background to different causes, $E_{n_E+1}$ ($\\:T$) can consist of any number of potentially distinguishable effects depending on how the lost events are distributed across the complement of $\\cup(E_1,E_2,\\dots,E_{n_E})$ in our effect phase space.}"}
include_graphics("problinks.png", dpi=275)
```

A visualization of these lost events can be seen in Figure [\ref{c2elinks}], where some collection of undocumented effects resulting from our collection of causes are lumped into a composite effect $E_{n_E+1}$, which should relate to our efficiency regarding cause $C_i$ by $P(E_{n_E+1}\vert C_i,I)=\lambda_{n_E+1,i}=1-\epsilon_i$. Including this new effect with the others results in $\sum_{j=1}^{n_E}\lambda_{ji}=1$, creating normalized basis vectors to define the columns of a \textit{smearing matrix} $\Lambda$, 
\begin{eqnarray}\Lambda
&=&\begin{pmatrix}
  P(E_1\vert C_1,I)       & P(E_1\vert C_2,I)       & \dots  & P(E_1\vert C_{n_C},I)       \\
  P(E_2\vert C_1,I)       & P(E_2\vert C_2,I)       & \cdots & P(E_2\vert C_{n_C},I)       \\
  \vdots                  & \vdots                  & \ddots & \vdots                      \\
  P(E_{n_E}\vert C_1,I) & P(E_{n_E}\vert C_2,I) & \dots  & P(E_{n_E}\vert C_{n_C},I) \\
  P(E_{n_E+1}\vert C_1,I) & P(E_{n_E+1}\vert C_2,I) & \dots  & P(E_{n_E+1}\vert C_{n_C},I)
  \end{pmatrix}\\
&=& (\bm\lambda_1,\bm\lambda_2,\dots,\bm\lambda_{n_c}),\nonumber
\end{eqnarray}
where $\bm\lambda_i$ refers to the $i$-th column consisting of $\{\lambda_{1,i},\lambda_{2,i},\dots,\lambda_{n_E+1,i}\}$.

Now that Eq. (2) accounts for lost events we can begin to construct a conditional probability for $\bm{x}_C$ similar to that for Eq. (1) using Bayes' theorem,
\begin{eqnarray}
P(\bm{x}_C\vert \bm{x}_E,\Lambda,I)\propto P(\bm{x}_E\vert \bm{x}_C,\Lambda,I)\cdot P(\bm{x}_C\vert I),
\end{eqnarray}
and account for uncertainties in $\Lambda$ with
$$P(\bm{x}_C\vert\bm{x}_E,I)=\int P(\bm{x}_C\vert \bm{x}_E,\Lambda,I)\ f(\Lambda\vert I)\ \text{d}\Lambda.$$
At this point one should recognize in Eq. (3) $P(\bm{x}_E\vert \bm{x}_C,\Lambda,I)$ as the likelihood and $P(\bm{x}_C\vert I)$ as the prior in the formal calculation of a posterior. Ignoring the prior for now and focusing on the likelihood, for a given cause $C_i$ we can model this expression as a multinomial distribution such that
\begin{eqnarray}
P(\bm{x}_E\vert x(C_i),\Lambda,I) &=& \frac{x(C_i)!}{\prod_j^{n_E+1}x(E_j)!}\prod_j^{n_E+1}\lambda_{ji}^{x(E_j)},
\end{eqnarray}
which leads us finally to asking how we should estimate our $\lambda_{ji}$'s. Once again, using Bayes' theorem we can see that for a fixed $i$,
\begin{eqnarray}
f(\bm{\lambda}_i\vert \bm{x}_E,x(C_i),I) &\propto& P(\bm{x}_E\vert x(C_i),\bm{\lambda}_i,I)\cdot f(\bm{\lambda}_i\vert I).
\end{eqnarray}
Previously mentioned properties about $\bm\lambda_i$ make a Dirichlet$(\bm\alpha_{prior_i})$ prior appropriate. A flat prior in which $\bm\alpha_{prior_i}=\{1,\dots,1\}$ is often chosen, and is done so here. Regardless of the choice for $\bm\alpha_{prior_i}$, multiplying by a multinomial distribution results in the posterior
\begin{eqnarray}
f(\bm{\lambda}_i\vert \bm{x}_E,x(C_i),I) &\propto& \left[\frac{x(C_i)!}{\prod_j^{n_E+1}x(E_j)!}\prod_j^{n_E+1}\lambda_{ji}^{x(E_j)}\right]\cdot\left[\frac{1}{\text{B}(\bm\alpha_{prior_i})}\prod_{j=1}^{n_E+1}\lambda_{ji}^{\alpha_{prior_{ji}}-1}\right] \nonumber\\
&\propto&\prod_j^{n_E+1}\lambda_{ji}^{\left(\alpha_{prior_{ji}}+x(E_j)\right)-1} \nonumber\\
&=& \text{Dirichlet}(\bm\alpha_{prior_i}+\bm{x}_E).
\end{eqnarray}
Samples from this distribution informs much of the the uncertainty resulting from the unfolding process, creating distributions for objects fully or partially calculated from them, such as the smearing matrix, efficiency, and inverse probabilities $\theta_{ij}$ once a prior for $P(C_i\vert I)$ is made. These will be necessary, as $P(\bm{x}_C\vert\bm{x}_E,I)$ becomes the sum of independent multinomial distributions, which does not have a closed solution that we can analytically maximize the likelihood of. We have to make the rest of our progress starting from Eq. (1) where the choice around a prior for $P(C_i\vert I)$ is due. The choice of $P(C_i\vert I)=constant$ is considered here, which D’Agostini acknowledges is a strong prior that produces biases that will require iterations to be resolved.

Defining $\bm\theta_{j}=\{\theta_{1,j},\theta_{2,j},\dots,\theta_{n_C,j}\}$, we can model how the $x(E_j)$ observed events with the effect $E_j$ are likely distributed from potential causes by the multinomial distribution
$$\bm{x}_C\vert_{x(E_j)}\;\sim\;\text{Mult}(x(E_j),\bm\theta_{j}).$$
Before summing over the effects to get the total observed causes we should acknowledge that each $x(E_j)$ is the result of a Poisson process with an unknown rate parameter $\mu_j$. Using the conjugate prior $\mu_j\;\sim\;\text{Gamma}(c_j,r_j)$, with $c_j=1$ and very small $r_j$ to create a flat prior, we arrive at 
$$\mu_j\vert_{x(E_j)}\;\sim\;\text{Gamma}(c_j+x(E_j),r_j+1),$$
which tells us not to use $x(E_j)$, but $\mu_j$. In dealing with fractional values of $\mu_j$ D’Agostini suggests:
\begin{enumerate}
  \item Rounding $\mu_j$ to its nearest positive integer $m_j$,
  \item Sampling from $\bm{x}_C\vert_{m_j}\;\sim\;\text{Mult}(m_j,\bm\theta_{j})$,
  \item Rescaling by $\bm{x}_C\vert_{\mu_j}=\frac{\mu_j}{m_j}\bm{x}_C\vert_{m_j}$,
  \item Summing over each effect with $\bm{x}_C\vert_{\bm{x}_E}=\sum_{j=1}^{n_E}\bm{x}_C\vert_{\mu_j}$,
  \item And applying the inefficiency correction with $\bm{x}_C=\frac{\bm{x}_C\vert_{\bm{x}_E}}{\epsilon_i}$.
\end{enumerate}
The drawing of multiple samples from the posteriors is used to form an ensemble of values of $\bm{x}_C$ and estimate credible intervals. The performance of second and later iterations is accomplished by using the previous iteration's posteriors as the new iteration's priors. 

\section{A Basic Example}

In the following example 100,000 random samples are drawn from a Cauchy distribution and then subject to some processes that disperse, bias, and reduce event selection efficiency. The results of these simulations are shown in Figure [\ref{exampleUnfold}]. The migration matrix does a decent job of demonstrating how the true data was smeared. For unaffected data one would see just a diagonal line from the bottom left to the top right. It was with this in mind that instead of choosing the earlier mentioned flat prior for $\bm\alpha_{prior_i}$ I decided to go with 
\begin{eqnarray}
  \alpha_{ij} = e^{-\vert x_{truth}-x_{smeared}\vert}\nonumber,
\end{eqnarray}
the values of which are represented in Figure [\ref{prior}].

```{r, fig.height=7.5, fig.align='center', fig.width=5, echo=F, fig.cap="\\label{exampleUnfold}\\emph{TOP: The true and smeared distribution of our toy model. BOTTOM: When considering count data, the smearing matrix is often referred to as the response or migration matrix. The bottom row corresponds to the events that were not assigned to an effect.}"}
nsim <- 100000

true <- rcauchy(nsim, 0.5, 1)
eff <-  runif(nsim) > 0.2 + (1.0-0.5)/20*(true+10.0)

predicted <- (true + rnorm(nsim, -2.5, 0.2))[eff]
step_hist_notfolded <- hist(true[which(true>-10 & true<10)], 
                           breaks = seq(-10,10,0.5), plot = F)
step_hist_folded <- hist(predicted[which(predicted>-10 & predicted<10)], 
                         breaks = seq(-10,10,0.5), plot = F)


ymax <- 1.075*max(c(step_hist_folded$density,step_hist_notfolded$density))
ymax_count <- 1.1*max(c(step_hist_folded$count,step_hist_notfolded$count))

step_hist <- 
  tibble(binLow = c(step_hist_folded$breaks[-length(step_hist_folded$breaks)],
                    step_hist_notfolded$breaks[-length(step_hist_notfolded$breaks)]),
         binHigh = c(step_hist_folded$breaks[-1],step_hist_notfolded$breaks[-1]),
         CountsL = c(c(0,step_hist_folded$counts[-length(step_hist_folded$counts)]),
                     c(0,step_hist_notfolded$counts[-length(step_hist_notfolded$counts)])),
         Counts = c(step_hist_folded$counts,step_hist_notfolded$counts),
         DensityL = c(c(0,step_hist_folded$density[-length(step_hist_folded$density)]),
                      c(0,step_hist_notfolded$density[-length(step_hist_notfolded$density)])),
         Density = c(step_hist_folded$density,step_hist_notfolded$density),
         Treatment = c(rep("smeared", length(step_hist_folded$density)),
                       rep("truth", length(step_hist_notfolded$density))))

dists <- ggplot(data = step_hist) + 
  theme_bw() +
  geom_segment(mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                          color = Treatment),
               alpha=0.6) +
  geom_segment(mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                          color = Treatment), 
               alpha=0.6) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts", limits = c(0,ymax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(ymax_count), ceiling(ymax_count/1000)*100)) +
  labs(x="X", y="", title=element_blank()) +
  scale_color_manual(values=c("red","blue")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
 
smear <- tibble(`True X` = c(true[eff],true[-eff]),
                `Observed X` = c(predicted,rep(-10.249999999,length(true[-eff]))))

smear_m <- ggplot(filter(smear, `True X` <= 10 & `Observed X` <= 10, `True X` >= -10 & 
                           (`Observed X` >= -10 | `Observed X` == -10.249999999))) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous(breaks = seq(-12,10,2), 
                     limits = c(-10.5,10), 
                     expand = c(0,0)) +
  theme_bw() + theme(panel.grid.major = element_blank(), 
                     panel.grid.minor = element_blank()) +
  scale_fill_gradientn(colours=r, breaks=seq(0,12000,2000)) +
  stat_bin2d(mapping=aes(x=`True X`, y=`Observed X`), 
             breaks = list(x=seq(-10,10,0.5),
                           y=seq(-10.5,10,0.5))) +
  geom_abline(slope = 0,intercept = -10,lty=2)

grid.arrange(dists, smear_m, nrow=2)
```

```{r, fig.height=3.75, fig.align='center', fig.width=4.5, echo=F, fig.cap="\\label{prior}\\emph{Instead of choosing a flat prior Dirichlet($\\bm\\alpha_{prior_i}$) in which $\\bm\\alpha_{prior_i}=\\{1,\\dots,1\\}$, I opted for one that assumed a dominant diagonal signal, corresponding to a max value of alpha along the diagonal that decays exponentially the further you get from the diagonal.}"}
test <- tibble(x = rep(seq(-9.5,9.5,1),20),
               y = rep(seq(-9.5,9.5,1),each=20))
test$scale <- exp(-abs(test$x-test$y))

rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(40)
ggplot(test) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  theme_bw() + theme(panel.grid.major = element_blank(), 
                     panel.grid.minor = element_blank()) +
  labs(x="True X", y="Observed X", title=expression(alpha ~"values for prior")) +
  scale_fill_gradientn(colours=r, breaks=seq(0,1,0.2)) +
  geom_tile(mapping=aes(x=x, y=y, fill=scale))
```


```{r, echo = F}     
# Number of "causes" and "effects"
nc <- ne <-  40

# Get migration counts from stored ggplot and create smearing matrix
smear_data <- ggplot_build(smear_m)$data[[1]]
smearing_matrix <- matrix(rep(0,nc*(ne+1)), ncol = nc)
for( i in 1:nrow(smear_data)){
  smearing_matrix[cbind(smear_data$xbin, (ne+2)-smear_data$ybin)[i,2],
                  cbind(smear_data$xbin, (ne+2)-smear_data$ybin)[i,1]] <- smear_data$value[i]
}
smearing_matrix_normcols <- smearing_matrix / 
  matrix(rep(apply(smearing_matrix, MARGIN = 2, sum),ne+1), nrow=ne+1, byrow = T)

# Get observed number of events per effect
xE1 <- smearing_matrix %*% rep(1,nc)

# Number of samples drawn from each posterior distribution
nsamp <- 10
Lamb1 <- theta1 <- invMig1 <- list(NA)
xC1 <- efficiency1 <- mu1 <- intmu1 <- matrix(rep(NA,nsamp*nc), ncol=nsamp)
for(i in 1:nsamp){
  # New smearing matrices sampled from Dirichlet posterior
  Lamb1[[i]] <- matrix(unlist(map(1:nc, ~ t(rdirichlet(1, exp(-abs(.-1:(ne+1)))+
                                                             smearing_matrix[,.])))), nrow=ne+1)
  # Efficiencies calculated for each 
  efficiency1[,i] <- apply(Lamb1[[i]][-(ne+1),], MARGIN = 2, sum)
  # Apply Bayes' theorem to get P(C|E)
  theta1[[i]] <- t(Lamb1[[i]][-(ne+1),]/(matrix(rep(apply(Lamb1[[i]][-(ne+1),],
                                                          MARGIN = 1, sum), nc), ncol=nc)))
  mu1[,i] <- map_dbl(xE1[-(ne+1)], ~ rgamma(1,1+.,1))
  intmu1[,i] <- round(mu1[,i])
  intmu1[,i][which(intmu1[,i]==0)] <- 1
  invMig1[[i]] <- matrix(unlist(map(1:ne, ~ rmultinom(n=1,size=intmu1[.,i], 
                                                     prob=theta1[[i]][,.]))),nrow=ne)
  xC1[,i] <- (apply(invMig1[[i]],MARGIN=1, sum)/
                efficiency1[,i])*(mu1[,i]/intmu1[,i])
}

step_hist$barlow <- rep(0,nc+ne)
step_hist$barhigh <- rep(0,nc+ne)

step <- filter(step_hist, Treatment == "truth")
step$Counts <- apply(xC1, MARGIN=1, mean)
step$CountsL[-1] <- apply(xC1, MARGIN=1, mean)[-nc]
step$Treatment <- "1 iteration"
step$Density <- apply(xC1, MARGIN=1, mean)/sum(apply(xC1, MARGIN=1, mean))
step$DensityL[-1] <- step$Density[-nc]
step$barlow <- apply(xC1, MARGIN=1, FUN=quantile, probs=0.025)
step$barhigh <- apply(xC1, MARGIN=1, FUN=quantile, probs=0.975)
step_hist_iter1 <- rbind(step_hist,step)

ymax_count <- 1.1*max(step_hist_iter1$Counts)

it1 <- ggplot(data = step_hist_iter1) + 
  theme_bw() +
  geom_rect(mapping = aes(xmin = binLow,
                          ymin = barlow,
                          xmax = binHigh,
                          ymax = barhigh,
                          fill = Treatment),
            alpha=0.2) +
  geom_segment(mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                          color = Treatment),
               alpha=0.7) +
  geom_segment(mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                          color = Treatment), 
               alpha=0.7) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts", limits = c(0,ymax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(ymax_count), ceiling(ymax_count/1000)*100)) +
  labs(x="X", y="", title="1 Iteration") +
  scale_color_manual(values=c("orange","red","blue")) +
  scale_fill_manual(values=c("orange","red","blue")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

```{r, echo = F}
# Get observed number of events per effect
xE2 <- xC1

# Number of samples drawn from posterior distribution
Lamb2 <- efficiency2 <- theta2 <- invMig2 <- list(NA)
xC2 <- efficiency2 <- mu2 <- intmu2 <- matrix(rep(NA,nsamp*nsamp*nc), ncol=nsamp*nsamp)
for(i in 1:nsamp){
  for(j in 1:nsamp){
    # New smearing matrices sampled from Dirichlet posterior
    Lamb2[[(i-1)*nsamp+j]] <- 
      matrix(unlist(map(1:nc, ~ t(rdirichlet(1,exp(-abs(.-1:(ne+1)))+smearing_matrix[,.]+c(invMig1[[i]][.,],0))))), 
             nrow=ne+1)
    # Efficiencies calculated for each 
    efficiency2[,(i-1)*nsamp+j] <- apply(Lamb2[[(i-1)*nsamp+j]][-(ne+1),],MARGIN=2, sum)
    # Apply Bayes' theorem to get P(C|E)
    theta2[[(i-1)*nsamp+j]] <- t((Lamb2[[(i-1)*nsamp+j]][-(ne+1),]*matrix(rep(xC1[,i], nc),byrow=T,ncol=nc))/
                                (matrix(rep(Lamb2[[(i-1)*nsamp+j]][-(ne+1),] %*% 
                                              xC1[,i], nc), ncol=nc)))
  
    mu2[,(i-1)*nsamp+j] <- map_dbl(1:ne, ~ rgamma(1,1+xE1[.]+xE2[.,j],2))
    intmu2[,(i-1)*nsamp+j] <- round(mu2[,(i-1)*nsamp+j])
    intmu2[,(i-1)*nsamp+j][which(intmu2[,(i-1)*nsamp+j]==0)] <- 1
    invMig2[[(i-1)*nsamp+j]] <- matrix(unlist(map(1:ne,~rmultinom(n=1,size=intmu2[.,(i-1)*nsamp+j], 
                                                                  prob=theta2[[(i-1)*nsamp+j]][,.]))),nrow=ne)
    xC2[,(i-1)*nsamp+j] <- (apply(invMig2[[(i-1)*nsamp+j]],MARGIN=1,sum)/efficiency2[,(i-1)*nsamp+j])*
      (mu2[,(i-1)*nsamp+j]/intmu2[,(i-1)*nsamp+j])
  }
}

step <- filter(step_hist, Treatment == "truth")
step$Counts <- apply(xC2, MARGIN=1, mean)
step$CountsL[-1] <- apply(xC2, MARGIN=1, mean)[-nc]
step$Treatment <- "2 iterations"
step$Density <- apply(xC2, MARGIN=1, mean)/sum(apply(xC2, MARGIN=1, mean))
step$DensityL[-1] <- step$Density[-nc]
step$barlow <- apply(xC2, MARGIN=1, FUN=quantile, probs=0.025)
step$barhigh <- apply(xC2, MARGIN=1, FUN=quantile, probs=0.975)
step_hist_iter2 <- rbind(step_hist,step)

ymax_count <- 1.1*max(step_hist_iter2$Counts,step_hist_iter2$barhigh)

it2 <- ggplot(data = step_hist_iter2) + 
  theme_bw() +
  geom_rect(mapping = aes(xmin = binLow,
                          ymin = barlow,
                          xmax = binHigh,
                          ymax = barhigh,
                          fill = Treatment),
            alpha=0.2) +
  geom_segment(mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                          color = Treatment),
               alpha=0.7) +
  geom_segment(mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                          color = Treatment), 
               alpha=0.7) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts", limits = c(0,ymax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(ymax_count), ceiling(ymax_count/1000)*100)) +
  labs(x="X", y="", title="2 Iterations") +
  scale_color_manual(values=c("orange","red","blue")) +
  scale_fill_manual(values=c("orange","red","blue")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

```{r, echo = F, warning=F}
# Get observed number of events per effect
xE3 <- xC2

# Number of samples drawn from posterior distribution
Lamb3 <- efficiency3 <- theta3 <- invMig3 <- list(NA)
xC3 <- efficiency3 <- mu3 <- intmu3 <- matrix(rep(NA,nc*nsamp^3), ncol=nsamp^3)
for(i in 1:nsamp){
  for(j in 1:nsamp){
    for(k in 1:nsamp){
      # New smearing matrices sampled from Dirichlet posterior
      Lamb3[[(i-1)*nsamp^2+(j-1)*nsamp+k]] <- 
        matrix(unlist(map(1:nc, ~ t(rdirichlet(1,exp(-abs(.-1:(ne+1)))+smearing_matrix[,.]+
                                                 c(invMig1[[i]][.,],0)+c(invMig2[[(j-1)*nsamp+k]][.,],0))))), 
               nrow=ne+1)
      # Efficiencies calculated for each 
      efficiency3[,(i-1)*nsamp^2+(j-1)*nsamp+k] <- 
        apply(Lamb3[[(i-1)*nsamp^2+(j-1)*nsamp+k]][-(ne+1),],MARGIN=2, sum)
      # Apply Bayes' theorem to get P(C|E)
      theta3[[(i-1)*nsamp^2+(j-1)*nsamp+k]] <- 
        t((Lamb3[[(i-1)*nsamp^2+(j-1)*nsamp+k]][-(ne+1),]*
             matrix(rep(xC3[,(j-1)*nsamp+k], nc),byrow=T,ncol=nc))/
            (matrix(rep(Lamb3[[(i-1)*nsamp^2+(j-1)*nsamp+k]][-(ne+1),] %*% 
                          xC3[,(j-1)*nsamp+k], nc), ncol=nc)))
    
      mu3[,(i-1)*nsamp^2+(j-1)*nsamp+k] <- 
        map_dbl(1:ne, ~ rgamma(1,1+xE1[.]+xE2[.,k]+xE3[(j-1)*nsamp+k],3))
      intmu3[,(i-1)*nsamp^2+(j-1)*nsamp+k] <- 
        round(mu3[,(i-1)*nsamp^2+(j-1)*nsamp+k])
      intmu3[,(i-1)*nsamp^2+(j-1)*nsamp+k][which(intmu3[,(i-1)*nsamp^2+(j-1)*nsamp+k]==0)] <- 1
      invMig3[[(i-1)*nsamp^2+(j-1)*nsamp+k]] <- 
        matrix(unlist(map(1:ne,~rmultinom(n=1,size=intmu3[.,(i-1)*nsamp^2+(j-1)*nsamp+k], 
                                          prob=theta2[[(i-1)*nsamp+j]][,.]))),nrow=ne)
      xC3[,(i-1)*nsamp^2+(j-1)*nsamp+k] <- 
        (apply(invMig3[[(i-1)*nsamp^2+(j-1)*nsamp+k]],MARGIN=1,sum)/efficiency3[,(i-1)*nsamp^2+(j-1)*nsamp+k])*
        (mu3[,(i-1)*nsamp^2+(j-1)*nsamp+k]/intmu3[,(i-1)*nsamp^2+(j-1)*nsamp+k])
    }
  }
}

step <- filter(step_hist_iter1, Treatment == "truth")
step$Counts <- apply(xC3, MARGIN=1, mean)
step$CountsL[-1] <- apply(xC3, MARGIN=1, mean)[-nc]
step$Treatment <- "3 iterations"
step$Density <- apply(xC3, MARGIN=1, mean)/sum(apply(xC3, MARGIN=1, mean))
step$DensityL[-1] <- step$Density[-nc]
step$barlow <- apply(xC3, MARGIN=1, FUN=quantile, probs=0.025)
step$barhigh <- apply(xC3, MARGIN=1, FUN=quantile, probs=0.975)
step_hist_iter3 <- rbind(step_hist,step)

ymax_count <- 1.1*max(step_hist_iter3$Counts,step_hist_iter2$barhigh)

it3 <- ggplot(data = step_hist_iter3) + 
  theme_bw() +
  geom_rect(mapping = aes(xmin = binLow,
                          ymin = barlow,
                          xmax = binHigh,
                          ymax = barhigh,
                          fill = Treatment),
            alpha=0.2) +
  geom_segment(mapping = aes(x = binLow,
                             y = Counts,
                          xend = binHigh,
                          yend = Counts,
                          color = Treatment),
               alpha=0.7) +
  geom_segment(mapping = aes(x = binLow,
                             y = CountsL,
                          xend = binLow,
                          yend = Counts,
                          color = Treatment), 
               alpha=0.7) +
  scale_x_continuous(breaks = seq(-10,10,2), 
                     limits = c(-10,10), 
                     expand = c(0,0)) +
  scale_y_continuous("Counts", limits = c(0,ymax_count), expand = c(0,0),
                     breaks = seq(0,ceiling(ymax_count), ceiling(ymax_count/1000)*100)) +
  labs(x="X", y="", title="3 Iterations") +
  scale_color_manual(values=c("orange","red","blue")) +
  scale_fill_manual(values=c("orange","red","blue")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```
\section{Discussion of Results and Conclusion}
The results of my unfolding are shown below in Figure [\ref{iterations}]. The iterations get off to an okay start, but instead of converging they begin to behave erratically. The tails blowing up clearly comes from using random samples from a posterior distribution as basis for new priors, embedding events in places where there were zero before. I attempted to remedy this with a prior that disfavored events occurring far from the diagonal, as mentioned previously in the context of Figure [\ref{prior}]. It kept these tails from blowing up for at least the first iteration. In the future I will look more into options relating to this issue.

There is almost certainly an error in the code governing the 3rd iteration, resulting in a vastly larger confidence band. Instead of trying to fix this issue I think it would be better use of my time to study some similar working examples \cite{Burgard2021}.

```{r, fig.height=8, fig.align='center', warning=F, fig.width=4.5, echo=F, fig.cap="\\label{iterations}\\emph{The first iteration looks like itd heading the right direction. It is my belief that I have an error somehwere in my work.}"}
grid.arrange(it1,it2,it3, nrow=3)
```

\newpage

\appendix

\section{Derivations}

\subsection{Bin-by-bin}\label{bin}

\section{R Code}

The libraries I used are shown here
```{r, message = F, eval=F}
library(tidyverse)
library(gridExtra)
library(knitr)
library(RColorBrewer)
library(gtools)
```

\subsection{Code used for Figure [\ref{BasicExample}]}
